# 第四章 神经网络的学习

## 本章概要

1. 学习是指从训练数据中自动获取最优权重参数的过程。本章中，为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数

2. 神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数。这个损失函数可以使用任意函数，
   一般用均方误差和交叉熵误差等

3. 均方误差如下所示，y为神经网络输出，t为标签：
   $$
   E=\frac{1}{2}\sum_k(y_k-t_k)^2
   $$
   交叉熵误差：
   $$
   E=-\sum_kt_k\log y_k
   $$
   当处理一批数据的时候，交叉熵变成所有数据交叉熵的总和：
   $$
   E=-\frac{1}{N}\sum_n\sum_kt_{nk}\log y_{nk}
   $$

4. 之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为0，无法梯度下降，导致参数无法更新。识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行

5. 像$(\frac{df}{dx_0},\frac{df}{dx_1})$这样的由全部变量的偏导数汇总而成的向量称为梯度，梯度法如下所示

   <img src="images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144028972.png" alt="image-20211214144028972" style="zoom:100%;" />

6. 计算梯度：数值微分法，误差反向传播法，本章的数值微分法太慢了真的，下一章学习误差反向传播算法

```python
#当y的维度是1时，即单个数据的交叉熵，需要改变数据的形状
#当监督数据是one-hot时
def cross_entropy_error1(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
#当监督数据是标签形式（非one-hot表示，而是像“2”“7” 这样的标签）时，交叉熵误差可通过如下代码实现
#y[np.arange(batch_size), t]会生成NumPy数组[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]）
#而y为神经网络输出层，每一行是概率，比如【0.1 0.3 0.03 0.07 0.5】，这样子就可以直接取到标签对应的位置
def cross_entropy_error2(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

本章数值微分法了解即可

# 第五章 误差反向传播

## 本章概要

1. 用计算图解题的情况下，需要按如下流程进行：1.构建计算图2.在计算图上，从左向右进行计算。这里从左往右计算即为正向传播。计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。计算图的优点：局部性，可保存局部结果，反向传播高效计算导数

2. 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数，规则：加法不变，乘法颠倒

   <img src="images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144048413.png" alt="image-20211214144048413" style="zoom:80%;" />

   <img src="images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144057245.png" alt="image-20211214144057245" style="zoom:80%;" />

3. 反向传播过程都使用了局部导数的性质，例如sigmod函数，可拆分如下

   ![image-20211214144110557](images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144110557.png)

   简化如下

   ![image-20211214144121432](images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144121432.png)

   进一步整理可得到

   ![image-20211214144134909](images/%E7%AC%AC%E5%9B%9B%EF%BC%8C%E4%BA%94%E7%AB%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20211214144134909.png)

```python
#重点在于理解每一层的正向反向传播流程
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y
        return out
    def backward(self, dout):
        dx = dout * self.y # 翻转x和y，dout是上游传来的导数
        dy = dout * self.x
        return dx, dy
class AddLayer:
    def __init__(self):
        pass
    def forward(self, x, y):
        out = x + y
        return out
    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
class Relu:
    def __init__(self):
        self.mask = None
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
class Sigmoid:
    def __init__(self):
        self.out = None
    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

